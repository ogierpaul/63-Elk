# This file is a copy of the production file stored in "/usr/share/logstash/pipeline/pgtarget.conf"
# Accessible via the docker volume logstash/pipeline
# https://www.elastic.co/blog/how-to-keep-elasticsearch-synchronized-with-a-relational-database-using-logstash
# 1. Put es_target_mapping.json  to ElasticSearch  PUT /pgtarget <mapping>

input {
  jdbc {
     jdbc_connection_string => "jdbc:postgresql://postgres:5432/mydb"
     jdbc_driver_library => "/drivers/postgresql-42.2.18.jar"
     jdbc_user => "myuser"
     jdbc_password => "mypassword"
     jdbc_driver_class => "org.postgresql.Driver"
    jdbc_paging_enabled => true
    tracking_column => "unix_ts_in_secs"
    use_column_value => true
    tracking_column_type => "numeric"
    schedule => "*/5 * * * * *"
    statement => "SELECT * FROM target.to_elastic WHERE ( unix_ts_in_secs > :sql_last_value AND update_ts < NOW()) ORDER BY row_id ASC, update_ts ASC"
 }
}

filter {
  mutate {
    copy => { "row_id" => "[@metadata][_id]"}
    remove_field => ["id", "@version", "unix_ts_in_secs"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "pgtarget"
    document_id => "%{[@metadata][_id]}"
    doc_as_upsert => true
    user => "elastic"
    password => "changeme"
 }
}
